#!/usr/bin/env python3

import argparse
import os
import sys
import time

from collections import defaultdict
from os import path
from sys import stderr, argv
from subprocess import Popen, PIPE
from shutil import copyfile

from config import configs
from cmd import run_cmd

ROOT_DIR = os.path.dirname(os.path.realpath(__file__))

DATA_DIR = path.join(ROOT_DIR, 'data')

DATASET_PREFIX = 'dataset_'


def slurp(file_name):
    with open(file_name, 'r') as f:
        return f.read().strip()


# from rainbow
def make_reporter(verbosity, quiet, filelike):
    '''
    Returns a function suitible for logging use.
    '''
    if not quiet:
        def report(level, msg, *args):
            'Log if the specified severity is <= the initial verbosity.'
            if level <= verbosity:
                if len(args):
                    filelike.write(msg % args + '\n')
                else:
                    filelike.write('%s\n' % (msg, ))
    else:
        def report(level, msg, *args):
            '/dev/null logger.'
            pass

    return report


ERROR = 0
WARN = 1
INFO = 2
DEBUG = 3
log = make_reporter(WARN, False, sys.stderr)


def new_result_dir(name, ref):
    '''
    Returns a previously unused directory path.
    '''
    suffix = '0000'
    os.makedirs(DATA_DIR, exist_ok=True)
    dirs = sorted(
        [d for d in os.listdir(DATA_DIR) if d.startswith(DATASET_PREFIX)])
    suffixes = sorted(set([f.split('_')[-1] for f in dirs]))
    if len(suffixes) > 0:
        suffix = suffixes[-1]
    next_suffix = int(suffix) + 1
    if ref:
        kind = 'ref'
    else:
        kind = 'test'
    dirname = path.join(DATA_DIR, '%s%s_%s_%04d' % (DATASET_PREFIX, kind, name, next_suffix))
    os.makedirs(dirname)
    return dirname


def test_speed(*benchmarks, enabled_configs=None, ref=False, docker=True, mstat=True):
    '''
    Run SPEC, collecting results
    '''
    benchmark_list = ' '.join(benchmarks)
    log(DEBUG, 'benchmarks: %s (mstat? %s)' % (benchmark_list, mstat))

    for config in configs:
        if enabled_configs and config.name not in enabled_configs:
            continue

        if mstat:
            spec_config = 'Linux-%s-mstat.cfg' % config.name
        else:
            spec_config = 'Linux-%s.cfg' % config.name

        flags = '--tune=base'
        if not ref:
            flags += ' --size=ref --noreportable --iterations=3'

        runspec_cmd = 'runspec --config=%s %s %s' % (spec_config, flags, benchmark_list)
        cmd = 'cd /spec && . ./shrc && %s' % runspec_cmd
        if docker:
            result_dir = new_result_dir(config.dir_name, ref)
            flags = ''
            if mstat:
                os.makedirs(path.join(result_dir, 'mstat'))
                os.chmod(path.join(result_dir, 'mstat'), 0o777)
                flags +=  '--privileged '
            flags += '--rm -it --mount type=bind,source=%s,target=/spec/result' % result_dir
            cmd = "docker run %s %s /bin/sh -c '%s'" % (flags, config.docker_image, cmd)

        log(WARN, '$ %s' % (cmd,))
        result = run_cmd(cmd).join()
        if result.returncode != 0:
            log(ERROR, '%s', result.stdout.decode('utf8'))
            log(ERROR, '~~~~~')
            log(ERROR, '%s', result.stderr.decode('utf8'))

# docker run --rm -it --mount source=results-01,target=/spec/result bpowers/spec:native /bin/sh -c 'cd /spec && . ./shrc && runspec --config=Linux-libc.cfg --size=test --noreportable --tune=base --iterations=1 bzip2'

TESTS = {
    'speed': test_speed,
}


def main():
    global log

    parser = argparse.ArgumentParser(description='Test redis with different allocators.')
    parser.add_argument('-v', action='store_const', const=True, help='verbose output')
    parser.add_argument('--no-mstat', action='store_const', const=True, help='Disable mstat support')
    parser.add_argument('--no-ref', action='store_const', const=True, help='Whether this is a reference run')
    parser.add_argument('--no-docker', action='store_const', const=True, help='Do not use docker')
    parser.add_argument('--config', nargs='+', help='specific configs to run')
    parser.add_argument('--test', nargs='+', help='specific tests to run')
    parser.add_argument('benchmark', nargs='*', default=['int'], help='specific benchmarks to run')
    args = parser.parse_args()

    if args.v:
        log = make_reporter(DEBUG, False, sys.stderr)

    if not args.test:
        args.test = TESTS.keys()

    for test in args.test:
        TESTS[test](*args.benchmark, enabled_configs=args.config,
                    ref=not args.no_ref, docker=not args.no_docker, mstat=not args.no_mstat)


if __name__ == '__main__':
    sys.exit(main())
